{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9c78d5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259477d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee3162",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093c9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/mystic/Programming/mystic-bot/notebooks\"\n",
    "data_path = f\"{base_dir}/data/chessData.csv\"\n",
    "model_dir = f\"{base_dir}/models\"\n",
    "plot_dir = f\"{base_dir}/plots\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "GLOBAL_DATA = pd.read_csv(data_path, usecols=[\"FEN\", \"Evaluation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3638c00",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd02f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 CUDA device(s) available:\n",
      "  └─ [0] NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"✅ {device_count} CUDA device(s) available:\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"  └─ [{i}] {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"⚠️ CUDA not available, using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d0fae",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae9204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_tensor(fen: str) -> np.ndarray:\n",
    "    piece_map = {\n",
    "        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "    }\n",
    "\n",
    "    tensor = np.zeros((18, 8, 8), dtype=np.float32)\n",
    "    board = chess.Board(fen)\n",
    "\n",
    "    # --- Piece Planes (12 channels) ---\n",
    "    for square, piece in board.piece_map().items():\n",
    "        row = 7 - (square // 8)  # White at bottom\n",
    "        col = square % 8\n",
    "        tensor[piece_map[piece.symbol()], row, col] = 1.0\n",
    "\n",
    "    # --- Turn (White to move: 1) [1 cell] ---\n",
    "    tensor[12, 0, 0] = 1.0 if board.turn == chess.WHITE else 0.0\n",
    "\n",
    "    # --- En Passant Square [1 cell] ---\n",
    "    if board.ep_square is not None:\n",
    "        row = 7 - (board.ep_square // 8)\n",
    "        col = board.ep_square % 8\n",
    "        tensor[13, row, col] = 1.0\n",
    "\n",
    "    # --- Castling Rights [1 cell each] ---\n",
    "    tensor[14, 0, 0] = float(\n",
    "        board.has_kingside_castling_rights(chess.WHITE))  # WK\n",
    "    tensor[15, 0, 0] = float(\n",
    "        board.has_queenside_castling_rights(chess.WHITE))  # WQ\n",
    "    tensor[16, 0, 0] = float(\n",
    "        board.has_kingside_castling_rights(chess.BLACK))  # BK\n",
    "    tensor[17, 0, 0] = float(\n",
    "        board.has_queenside_castling_rights(chess.BLACK))  # BQ\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9fced",
   "metadata": {},
   "source": [
    "# Dataset Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f995653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_eval(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "class ChessPositionDataset(Dataset):\n",
    "    def __init__(self, max_samples=None):\n",
    "        df = GLOBAL_DATA.copy()\n",
    "        df = df.dropna(subset=[\"FEN\", \"Evaluation\"])\n",
    "\n",
    "        # Parse and normalize evaluation values\n",
    "        df[\"Evaluation\"] = df[\"Evaluation\"].apply(parse_eval)\n",
    "        df = df.dropna(subset=[\"Evaluation\"])\n",
    "\n",
    "        # Shuffle and trim\n",
    "        if max_samples:\n",
    "            df = df.sample(frac=1, random_state=42).iloc[:max_samples]\n",
    "\n",
    "        self.max_score = df[\"Evaluation\"].abs().max()\n",
    "        self.samples = df.to_dict(\"records\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        # Should be (18, 8, 8) np array\n",
    "        input_tensor = fen_to_tensor(sample[\"FEN\"])\n",
    "        input_tensor = torch.tensor(input_tensor, dtype=torch.float32)\n",
    "\n",
    "        eval_score = float(sample[\"Evaluation\"]) / self.max_score\n",
    "        eval_score = torch.tensor(eval_score, dtype=torch.float32)\n",
    "\n",
    "        return input_tensor, eval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882a03c",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e10751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load full dataset ---\n",
    "dataset = ChessPositionDataset(\n",
    "    max_samples=200_000  # You can change this based on memory/batch size\n",
    ")\n",
    "\n",
    "# --- Split sizes ---\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# --- Split dataset ---\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f0916",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a38a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 64, 8, 8]          10,432\n",
      "       BatchNorm2d-2             [-1, 64, 8, 8]             128\n",
      "              ReLU-3             [-1, 64, 8, 8]               0\n",
      "            Conv2d-4             [-1, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
      "              ReLU-6             [-1, 64, 8, 8]               0\n",
      "            Conv2d-7             [-1, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-8             [-1, 64, 8, 8]             128\n",
      "              ReLU-9             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 128, 8, 8]          73,856\n",
      "      BatchNorm2d-12            [-1, 128, 8, 8]             256\n",
      "             ReLU-13            [-1, 128, 8, 8]               0\n",
      "           Conv2d-14            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-15            [-1, 128, 8, 8]             256\n",
      "             ReLU-16            [-1, 128, 8, 8]               0\n",
      "           Conv2d-17            [-1, 128, 8, 8]         147,584\n",
      "      BatchNorm2d-18            [-1, 128, 8, 8]             256\n",
      "             ReLU-19            [-1, 128, 8, 8]               0\n",
      "    ResidualBlock-20            [-1, 128, 8, 8]               0\n",
      "           Conv2d-21            [-1, 256, 8, 8]         295,168\n",
      "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
      "             ReLU-23            [-1, 256, 8, 8]               0\n",
      "           Conv2d-24            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
      "             ReLU-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "             ReLU-29            [-1, 256, 8, 8]               0\n",
      "    ResidualBlock-30            [-1, 256, 8, 8]               0\n",
      "AdaptiveAvgPool2d-31            [-1, 256, 1, 1]               0\n",
      "          Flatten-32                  [-1, 256]               0\n",
      "           Linear-33                  [-1, 512]         131,584\n",
      "             ReLU-34                  [-1, 512]               0\n",
      "          Dropout-35                  [-1, 512]               0\n",
      "           Linear-36                  [-1, 128]          65,664\n",
      "             ReLU-37                  [-1, 128]               0\n",
      "          Dropout-38                  [-1, 128]               0\n",
      "           Linear-39                    [-1, 1]             129\n",
      "================================================================\n",
      "Total params: 2,128,705\n",
      "Trainable params: 2,128,705\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.21\n",
      "Params size (MB): 8.12\n",
      "Estimated Total Size (MB): 10.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.block(x))\n",
    "\n",
    "\n",
    "class EvalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(18, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "\n",
    "        # Global average pooling instead of full flatten\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gap(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "model = EvalNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "summary(model, input_size=(18, 8, 8), device=str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b3e152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Initial Validation (Untrained Model)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7408f2a3024ecf95494015d804b583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Initial Val Loss (Untrained): 0.0047\n"
     ]
    }
   ],
   "source": [
    "# Initial Validation (Untrained Model)\n",
    "model.eval()\n",
    "total_val_loss = 0.0\n",
    "val_steps = 0\n",
    "\n",
    "print(\"\\n🔎 Initial Validation (Untrained Model)...\")\n",
    "val_pbar = tqdm(val_loader, desc=\"Validation\",\n",
    "                unit=\"batch\", total=len(val_loader))\n",
    "with torch.no_grad():\n",
    "    for x, y in val_pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).squeeze()\n",
    "        loss = loss_fn(pred, y.squeeze())\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        val_steps += 1\n",
    "        val_pbar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "initial_val_loss = total_val_loss / val_steps\n",
    "print(f\"\\n🧪 Initial Val Loss (Untrained): {initial_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811960ed",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0735326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📚 Epoch 1/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f80f13200a3480895378af77b277a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 1/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344b32a75fc14be5b8ab4345225c39df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1 Summary | Train Loss: 0.0040 | Val Loss: 0.0037\n",
      "\n",
      "📚 Epoch 2/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fdcfac5ef44f2d9d2d7686dcee3815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 2/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36158b2241b450f944464af1c91849b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 2 Summary | Train Loss: 0.0035 | Val Loss: 0.0037\n",
      "\n",
      "📚 Epoch 3/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260cfdf651144e29be0befcaf6986c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 3/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862edbf23e0e489cac00f59109ad79ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 3 Summary | Train Loss: 0.0033 | Val Loss: 0.0035\n",
      "\n",
      "📚 Epoch 4/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1519733a4e4f4c09a7132c13a87dc18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 4/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f66b8f49d94f3092e40ef0b0c80329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 4 Summary | Train Loss: 0.0029 | Val Loss: 0.0038\n",
      "\n",
      "📚 Epoch 5/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6e9a6234144e8d9809f9e2479fe9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 5/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66001136abc453986b25a34c70277a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 5 Summary | Train Loss: 0.0023 | Val Loss: 0.0039\n",
      "\n",
      "📚 Epoch 6/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79a8044902d4e1dbce996906224052d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 6/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc35f49b8d4d4210a9065550dc69c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 6 Summary | Train Loss: 0.0016 | Val Loss: 0.0039\n",
      "\n",
      "📚 Epoch 7/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3628de69de9f463c875e2d6fb32a1a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 7/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685a3bc5d9f34e43a465874a4ca1dfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 7 Summary | Train Loss: 0.0013 | Val Loss: 0.0042\n",
      "\n",
      "📚 Epoch 8/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f566d051bab746d9bd6a2978136a3128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 8/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e28785594dd4178b03e343d6d1c8c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 8 Summary | Train Loss: 0.0011 | Val Loss: 0.0045\n",
      "\n",
      "📚 Epoch 9/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3486da5b980944f499038d6f01803e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 9/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585ddb7f737f400d9a9971dd12972e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 9 Summary | Train Loss: 0.0010 | Val Loss: 0.0038\n",
      "\n",
      "📚 Epoch 10/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cc81f2e6364977b3713f776a96c5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Epoch 10/50 - Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74b82a747774907ba8335855920b766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 10 Summary | Train Loss: 0.0009 | Val Loss: 0.0037\n",
      "\n",
      "📚 Epoch 11/50 - Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdec2e2b7814dc6bda46c318b880d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📚 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m             unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[0;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mChessPositionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Should be (18, 8, 8) np array\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mfen_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_tensor, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     33\u001b[0m eval_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_score\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mfen_to_tensor\u001b[0;34m(fen)\u001b[0m\n\u001b[1;32m     12\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m-\u001b[39m (square \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m)  \u001b[38;5;66;03m# White at bottom\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     col \u001b[38;5;241m=\u001b[39m square \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     tensor[piece_map[\u001b[43mpiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbol\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m], row, col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# --- Turn (White to move: 1) [1 cell] ---\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tensor[\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m board\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;241m==\u001b[39m chess\u001b[38;5;241m.\u001b[39mWHITE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/Programming/mystic-bot/.env/lib/python3.12/site-packages/chess/__init__.py:596\u001b[0m, in \u001b[0;36mPiece.symbol\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m color: Color\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The piece color.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbol\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    597\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m    Gets the symbol ``P``, ``N``, ``B``, ``R``, ``Q`` or ``K`` for white\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    pieces or the lower-case variants for the black pieces.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m     symbol \u001b[38;5;241m=\u001b[39m piece_symbol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpiece_type)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "    print(f\"\\n📚 Epoch {epoch+1}/{epochs} - Training...\")\n",
    "    pbar = tqdm(train_loader, desc=\"Training\",\n",
    "                unit=\"batch\", total=len(train_loader))\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).squeeze()\n",
    "        loss = loss_fn(pred, y.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_steps += 1\n",
    "        pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_train_loss / train_steps\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_steps = 0\n",
    "\n",
    "    print(f\"\\n🔎 Epoch {epoch+1}/{epochs} - Validation...\")\n",
    "    val_pbar = tqdm(val_loader, desc=\"Validation\",\n",
    "                    unit=\"batch\", total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x).squeeze()\n",
    "            loss = loss_fn(pred, y.squeeze())\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "            val_pbar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Logging\n",
    "    print(\n",
    "        f\"\\n✅ Epoch {epoch+1} Summary | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Checkpointing\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, f\"{timestamp}.pt\"))\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, \"best.pt\"))\n",
    "\n",
    "    # Plotting (no display)\n",
    "    plt.figure(figsize=(12, 6), dpi=150)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Final Evaluation\n",
    "print(\"\\n🔍 Loading best model for final test...\")\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, \"best.pt\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c1ce1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3033d064234746af05889d7d55886a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/313 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Final Test Loss: 0.0072\n"
     ]
    }
   ],
   "source": [
    "total_test_loss = 0.0\n",
    "test_steps = 0\n",
    "\n",
    "print(\"\\n🧪 Evaluating on test set...\")\n",
    "test_pbar = tqdm(test_loader, desc=\"Testing\",\n",
    "                 unit=\"batch\", total=len(test_loader))\n",
    "with torch.no_grad():\n",
    "    for x, y in test_pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).squeeze()\n",
    "        loss = loss_fn(pred, y.squeeze())\n",
    "\n",
    "        total_test_loss += loss.item()\n",
    "        test_steps += 1\n",
    "        test_pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "avg_test_loss = total_test_loss / test_steps\n",
    "print(f\"\\n🎯 Final Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0f08d",
   "metadata": {},
   "source": [
    "# Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afddf632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Evaluation: 0.3852\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fen(model, fen):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tensor = torch.tensor(fen_to_tensor(\n",
    "            fen), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        pred = model(tensor).squeeze().item()\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "fen = \"8/8/8/P7/4pk2/2R3p1/5PK1/4B3 b - - 0 44\"\n",
    "score = evaluate_fen(model, fen)*100\n",
    "print(f\"Predicted Evaluation: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115043e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
